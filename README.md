# Sparkify Spark Datalake

## Purpose of the database

- A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
- The analytics team is particularly interested in understanding what songs users are listening to. 
- Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Data used

- Song data: 's3://udacity-dend/song_data'
- Log data: 's3://udacity-dend/log_data'

### Song File

Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID

And below is an example of what a single song file looks like

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### log file

- The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

- The log files in the dataset are partitioned by year and month.

![Alt Text](.\log-data.png)

## Solution

-  Make ETL Pipeline to extract the song and log files
-  Create a Postgres database with tables designed to optimize queries on song play analysis 
-  Test the database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Database Schema

- The database schema is star schema to implement queries faster as below
    - The Fact Table is songplays table
    - The dimension tables are (users, songs, artists and time)


## ETL Pipeline

After creating the database the ETL pipeline consists of:

- getting the log and song data from s3 bucket
- inserting the song data into the **songs and artists** tables
- Converting the log data column (ts) into timestamp
- Making the different time formats (hour, week, day, etc) and inserting them into **time** table
- Inserting the data from the log data into the **songplays and users** tables

## Repo Files


- etl.py: is to 
	-get the data from s3 bucket
	-create the fact and dimensions tables and load the data into them
	-load the tables data into s3

- dl.cfg: has AWS Access Keys


## How to run

Run etl.py
